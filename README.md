# The Relationship Between Adversarial Robustness and Saliency


This project is the study of machine learning interpretability and robustness by conducting an examination of four distinct time series models, namely LSTM, LSTM with Input-Cell Attention, CNN, and Transformer, under the influence of the Fast Gradient Sign Method (FGSM) attack. Using the ItalyPowerDemand dataset, we investigate how these models respond to adversarial perturbations and how the saliencies evolve during such attacks. Saliency serves as a crucial indicator of the importance of different features within the input data, providing insights into the models' decision-making processes.

By subjecting each of these models to FGSM attacks, we aim to show the changes in saliency patterns. The FGSM attack, a well-known adversarial technique, manipulates input data to deceive models into making incorrect predictions. Our comparative study will show whether these attacks have a consistent impact across the different time series models and how this impact influences the saliency maps.

The findings from this project contribute to a deeper understanding of the relationship between adversarial attacks and model interpretability in the context of time series data, enabling researchers and practitioners to better comprehend the vulnerabilities and strengths of these models in real-world applications. 
\href{https://github.com/nkotelevskii/msd_sk_project}{GitHub Project Link} 
